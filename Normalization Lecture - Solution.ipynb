{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "Normalizing text is a critical component of text mining and a step we'll take on every single analysis. Eventually it'll get to the point that it's basically second nature. This notebook accompanies the lecture, where we mention six common types of text normalization: \n",
    "\n",
    "1. Case folding\n",
    "1. Removing punctuation\n",
    "1. Handling numbers, dates, and times\n",
    "1. Extracting special information\n",
    "1. Removing stopwords\n",
    "1. Correcting spelling\n",
    "\n",
    "We'll work through a few examples of most of these, although we'll save spelling correction for another day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Case Folding\n",
    "\n",
    "We'll often discover that having a mixture of upper and lower case doesn't serve us very well. Case folding helps us handle this. Let's start by finding all the words that appear in the top 1000 most frequent words in the chat corpus with multiple capitalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = text5\n",
    "chat_count = Counter(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use a dictionary to hold all the words in the top 1000. The key will be the lowercase word and the value will be a list of every word that maps onto that lowercase word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_collisions = dict() # make a set to hold lowercase versions\n",
    "\n",
    "for word, count in chat_count.most_common(1000) :\n",
    "    lc_word = word.lower()\n",
    "    \n",
    "    if lc_word not in case_collisions : \n",
    "        case_collisions[lc_word] = [word]\n",
    "    else :\n",
    "        case_collisions[lc_word].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words PART,part map onto part\n",
      "The words lol,LOL,LoL,Lol map onto lol\n",
      "The words i,I map onto i\n",
      "The words the,The map onto the\n",
      "The words you,You,YOU map onto you\n",
      "The words a,A map onto a\n",
      "The words hi,Hi map onto hi\n",
      "The words me,ME map onto me\n",
      "The words is,Is map onto is\n",
      "The words and,And map onto and\n",
      "The words it,It map onto it\n",
      "The words that,That map onto that\n",
      "The words hey,Hey map onto hey\n",
      "The words my,My map onto my\n",
      "The words what,What map onto what\n",
      "The words not,NOT map onto not\n",
      "The words do,Do map onto do\n",
      "The words no,No map onto no\n",
      "The words im,Im map onto im\n",
      "The words how,How map onto how\n",
      "The words pm,PM map onto pm\n",
      "The words lmao,LMAO map onto lmao\n",
      "The words who,Who map onto who\n",
      "The words if,If map onto if\n",
      "The words ok,OK map onto ok\n",
      "The words am,AM map onto am\n",
      "The words but,But map onto but\n",
      "The words this,This map onto this\n",
      "The words he,He map onto he\n",
      "The words well,Well map onto well\n",
      "The words m,M map onto m\n",
      "The words now,Now map onto now\n",
      "The words oh,Oh map onto oh\n",
      "The words hiya,Hiya map onto hiya\n",
      "The words they,They map onto they\n",
      "The words yeah,Yeah map onto yeah\n",
      "The words hello,Hello map onto hello\n",
      "The words yes,Yes map onto yes\n",
      "The words thanks,Thanks map onto thanks\n",
      "The words anyone,Anyone map onto anyone\n",
      "The words when,When map onto when\n",
      "The words she,She map onto she\n",
      "The words f,F map onto f\n",
      "The words song,Song map onto song\n",
      "The words welcome,Welcome map onto welcome\n",
      "The words omg,OMG map onto omg\n",
      "The words NICK,nick map onto nick\n",
      "The words last,Last map onto last\n",
      "The words please,Please map onto please\n",
      "The words music,Music map onto music\n",
      "The words Question,question map onto question\n"
     ]
    }
   ],
   "source": [
    "for word, wlist in case_collisions.items() :\n",
    "    if len(wlist) > 1 :\n",
    "        print(f'The words {\",\".join(wlist)} map onto {word}') \n",
    "        # using the new-ish f strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a slightly easier one, how many times are \"the\" and \"The\" used in _Moby Dick_? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13721"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text1)['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text1)['The']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Punctuation\n",
    "\n",
    "Punctuation can be tricky to handle. The easiest thing is to remove it, but that's not always the best thing to do. To practice playing around with it, count the number of **unique** words that have punctuation in them _Beowulf_. Print out a few to look at (although there are a lot, so maybe don't print them all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf = open(\"beowulf.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3478\n",
      "['flashes,', 'ether-robed', 'bearer.', 'murder,', 'anew.', 'good,', 'spoken,', 'ravens;', 'misery.', \"prince's\", 'he,', 'bold-hearted', 'asunder,', 'breath,', 'cavern-hall,', 'weary-hearted,', 'ash:', 'added,', 'saw,', '{28e}']\n"
     ]
    }
   ],
   "source": [
    "# Let's grab every word with punctuation. \n",
    "# One straightforward way to do this is to make punctuation a \n",
    "# set and intersect it with the set of characters in the word. \n",
    "\n",
    "punct_set = set(punctuation)\n",
    "punct_words = set() # since we want uniques\n",
    "\n",
    "for word in beowulf.split() :\n",
    "    wset = set(word)\n",
    "    if punct_set.intersection(wset) :\n",
    "        punct_words.add(word)\n",
    "    \n",
    "print(len(punct_words))\n",
    "\n",
    "# Let's print 20 or so\n",
    "print(list(punct_words)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3478\n"
     ]
    }
   ],
   "source": [
    "# While we're here, we can use the `isalnum` function to test if a string is alphanumeric. \n",
    "# This makes the code much simpler. There are also functions like isalpha and isnumeric\n",
    "# https://docs.python.org/3/library/stdtypes.html#str.isalpha\n",
    "punct_set_2 = set() \n",
    "\n",
    "for word in beowulf.split() :\n",
    "    if not word.isalnum() :\n",
    "        punct_set_2.add(word)\n",
    "\n",
    "print(len(punct_set_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of that punctuation is at the end of words (e.g., \"gallows.\" and \"vain;\"). Let's count the number of words that have punctuation in the _middle_ of the word. Let's also throw them in a `Counter` object and look at the most common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_mid_words = [] # Use a list so we can use a counter. \n",
    "\n",
    "for word in beowulf.split() :\n",
    "    if not word.isalnum() and len(word) > 1:\n",
    "        # now we're in the case of punctuation somewhere\n",
    "        # need to test if it's start or end. \n",
    "        if (not word[1] in punctuation and\n",
    "            not word[-1] in punctuation) :\n",
    "            punct_mid_words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(punct_mid_words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "There are many common words that don't help analysis that much (and can take up a lot of space). These are called stopwords. Let's play around with the English stopwords.\n",
    "1. Load in the English stopwords and assign them to a variable called `sw`. Print them out. Any surprises?\n",
    "1. Look at the top words in _Moby Dick_ and _Sense and Sensibility_.\n",
    "1. Look at the top words in both of those that _aren't_ stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713),\n",
       " ('the', 13721),\n",
       " ('.', 6862),\n",
       " ('of', 6536),\n",
       " ('and', 6024),\n",
       " ('a', 4569),\n",
       " ('to', 4542),\n",
       " (';', 4072),\n",
       " ('in', 3916),\n",
       " ('that', 2982)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 9397),\n",
       " ('to', 4063),\n",
       " ('.', 3975),\n",
       " ('the', 3861),\n",
       " ('of', 3565),\n",
       " ('and', 3350),\n",
       " ('her', 2436),\n",
       " ('a', 2043),\n",
       " ('I', 2004),\n",
       " ('in', 1904)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text2).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the same stats but without stopwords and non-alpha strings, I'm going to use a list comprehension. If you haven't seen these before, here's a nice [tutorial](https://www.youtube.com/watch?v=AhSvKGTh28Q). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 906),\n",
       " ('one', 889),\n",
       " ('like', 624),\n",
       " ('upon', 538),\n",
       " ('man', 508),\n",
       " ('ship', 507),\n",
       " ('Ahab', 501),\n",
       " ('ye', 460),\n",
       " ('old', 436),\n",
       " ('sea', 433)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([w for w in text1 if w.lower() not in sw and w.isalpha()]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elinor', 684),\n",
       " ('could', 568),\n",
       " ('Marianne', 566),\n",
       " ('Mrs', 530),\n",
       " ('would', 507),\n",
       " ('said', 397),\n",
       " ('every', 361),\n",
       " ('one', 304),\n",
       " ('much', 287),\n",
       " ('sister', 282)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([w for w in text2 if w.lower() not in sw and w.isalpha()]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process by which we move from a token to some \"root\" of that word. Let's explore one of the stemmers available through NLTK.\n",
    "\n",
    "First, let's find all the words in the NLTK words corpus that end in \"ing\", then let's find those that have no vowels before an instance of \"ing\". You can access the words corpus with the confusing call of `nltk.corpus.words.words()`. To make it easier to deal with \"y\", let's just consider it a vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.corpus.words.words()\n",
    "vowels = set('aeiouy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_words = [w for w in words if len(w) > 3 and w[-3:]==\"ing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5557"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bing',\n",
       " 'bring',\n",
       " 'ching',\n",
       " 'cling',\n",
       " 'ding',\n",
       " 'fling',\n",
       " 'ging',\n",
       " 'hing',\n",
       " 'Irving',\n",
       " 'jing',\n",
       " 'King',\n",
       " 'king',\n",
       " 'Kling',\n",
       " 'ling',\n",
       " 'Ming',\n",
       " 'ming',\n",
       " 'Ning',\n",
       " 'Ping',\n",
       " 'ping',\n",
       " 'ring',\n",
       " 'sing',\n",
       " 'sling',\n",
       " 'Spring',\n",
       " 'spring',\n",
       " 'sting',\n",
       " 'string',\n",
       " 'swing',\n",
       " 'thing',\n",
       " 'thring',\n",
       " 'Ting',\n",
       " 'ting',\n",
       " 'whing',\n",
       " 'wing',\n",
       " 'wring',\n",
       " 'zing',\n",
       " 'ring',\n",
       " 'spring',\n",
       " 'thing',\n",
       " 'wing']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's find the subset that don't have a vowel before the 'ing'\n",
    "ing_no_vowel = []\n",
    "\n",
    "for word in ing_words :\n",
    "    remainder = word[:-3]\n",
    "    if len(set(remainder).intersection(vowels))==0 :\n",
    "        ing_no_vowel.append(word)\n",
    "        \n",
    "ing_no_vowel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play around with the Porter Stemmer in NLTK. First we'll look at a few hundred characters of inaugural addresses both stemmed and not stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid of that Almighty Power which has hitherto protected me and enabled me to bring to favorable issues other important but still greatly inferior trusts heretofore confided to me by my country . The broad foundation upon which our Constitution rests being the people -- a breath of theirs having made , as a breath can unmake , change , or modify it -- it can be assigned to none of the great divisions of government but to that of democracy . If such is its theory , those who are called upon to administer it must recognize as its leading principle the duty of shaping their measures so as to produce the greatest good to the greatest number . But with these broad admissions , if we would compare the sovereignty acknowledged to exist in the mass of our people with the power claimed by other sovereignties , even by those which have been considered most purely democratic , we shall find a most essential difference . All others lay claim to power limited only by their own will . The majority of our citizens , on the contrary , possess a sovereignty with an amount of power precisely\n",
      "\n",
      "\n",
      "\n",
      "aid of that almighti power which ha hitherto protect me and enabl me to bring to favor issu other import but still greatli inferior trust heretofor confid to me by my countri . the broad foundat upon which our constitut rest be the peopl -- a breath of their have made , as a breath can unmak , chang , or modifi it -- it can be assign to none of the great divis of govern but to that of democraci . If such is it theori , those who are call upon to administ it must recogn as it lead principl the duti of shape their measur so as to produc the greatest good to the greatest number . but with these broad admiss , if we would compar the sovereignti acknowledg to exist in the mass of our peopl with the power claim by other sovereignti , even by those which have been consid most pure democrat , we shall find a most essenti differ . all other lay claim to power limit onli by their own will . the major of our citizen , on the contrari , possess a sovereignti with an amount of power precis\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 200\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for you: how many words are in the inaugural addresses? How many lowercase stems are in them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9913\n"
     ]
    }
   ],
   "source": [
    "# words in inaugural addresses\n",
    "print(len(set(text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5562\n",
      "1.7822725638259618\n"
     ]
    }
   ],
   "source": [
    "inaug_stemmed = {porter.stem(w.lower()) for w in text4}\n",
    "\n",
    "print(len(inaug_stemmed))\n",
    "\n",
    "print(len(set(text4))/len(inaug_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Okay, let's have some \"fun\" and play around with some sets of characters that aren't words. Text 5 is the chat corpus. Find the emojis in there (doesn't have to be perfect) and count up the happy and sad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we're catching some non-emojis, but let's assume we're getting most of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count happy vs sad\n",
    "happy = [w for w in chat if w in {\":-)\",\":)\",\":D\",\";-)\",\"=)\"}]\n",
    "sad = [w for w in chat if w in {\":-(\",\":(\",\";-(\",\"=(\"}]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
