{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the lectures on tokenization, normalization, and stemming. \n",
    "\n",
    "Before we get started, you're going to need the NLTK book corpus. Here are the steps to install it: \n",
    "\n",
    "1. Open a console or command window.\n",
    "1. Type `python` to start using python. \n",
    "1. Type `import nltk` and hit enter.\n",
    "1. Type `nltk.download()` and hit enter.\n",
    "1. This will open a little window. \n",
    "1. Click \"All Packages\" at the top of the list. \n",
    "1. Click \"Download\"\n",
    "\n",
    "Let me know if you run into any issues!\n",
    "\n",
    "---\n",
    "\n",
    "Now we can get started in earnest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process by which we split text up into tokens. The simplest tokens are those split by whitespace. Let's begin by counting the words in a file that I've included the in repo: the text of _Beowulf_. \n",
    "\n",
    "1. Read the file into a variable that holds a (large) string. \n",
    "1. Look at the first 1000 characters of that string.\n",
    "1. Split that string on whitespace (spaces, returns, tabs, etc.)\n",
    "1. Count the number of tokens. \n",
    "1. Determine the most common 10 tokens. \n",
    "1. Find the tokens that include punctuation within the token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf = open(\"beowulf.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"BEOWULF\\n\\nBy Anonymous\\n\\nTranslated by Gummere\\n\\n\\n\\n\\nBEOWULF\\n\\n\\n\\n\\nPRELUDE OF THE FOUNDER OF THE DANISH HOUSE\\n\\n\\n\\nLO, praise of the prowess of people-kings\\nof spear-armed Danes, in days long sped,\\nwe have heard, and what honor the athelings won!\\nOft Scyld the Scefing from squadroned foes,\\nfrom many a tribe, the mead-bench tore,\\nawing the earls. Since erst he lay\\nfriendless, a foundling, fate repaid him:\\nfor he waxed under welkin, in wealth he throve,\\ntill before him the folk, both far and near,\\nwho house by the whale-path, heard his mandate,\\ngave him gifts:  a good king he!\\nTo him an heir was afterward born,\\na son in his halls, whom heaven sent\\nto favor the folk, feeling their woe\\nthat erst they had lacked an earl for leader\\nso long a while; the Lord endowed him,\\nthe Wielder of Wonder, with world's renown.\\nFamed was this Beowulf:  {0a} far flew the boast of him,\\nson of Scyld, in the Scandian lands.\\nSo becomes it a youth to quit him well\\nwith his father's friends, by fee and gift,\\nthat to aid \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beowulf[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_tokens = beowulf.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beo_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to count things. The handiest is the `Counter` data type. I'll illustrate it's use and show you how you could do the same thing with a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the Counter version\n",
    "#from collections import Counter # Typically you'd do this at the top of the notebook.\n",
    "\n",
    "beo_counter = Counter(beo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1701),\n",
       " ('of', 1032),\n",
       " ('and', 689),\n",
       " ('to', 531),\n",
       " ('in', 452),\n",
       " ('his', 428),\n",
       " ('that', 322),\n",
       " ('he', 312),\n",
       " ('with', 286),\n",
       " ('was', 240)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can do things like find the most common tokens\n",
    "beo_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Counter` is really just a dictionary, where the keys are the elements of the list that you fed in, and the values are the integer counts. Here's how you'd do the same thing with a dictionary. Notice how much more difficult it is, and the weird construction to sort the dictionary by values for reading out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_dict = dict()\n",
    "\n",
    "for t in beo_tokens :\n",
    "    \n",
    "    # Have to create the spot in the dictionary if it's not in there.\n",
    "    if t not in beo_dict :\n",
    "        beo_dict[t] = 0\n",
    "    \n",
    "    # And now increment the count\n",
    "    beo_dict[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the had 1701 instances.\n",
      "of had 1032 instances.\n",
      "and had 689 instances.\n",
      "to had 531 instances.\n",
      "in had 452 instances.\n",
      "his had 428 instances.\n",
      "that had 322 instances.\n",
      "he had 312 instances.\n",
      "with had 286 instances.\n",
      "was had 240 instances.\n"
     ]
    }
   ],
   "source": [
    "# Printing out the top 10 is pretty tricky. Do the work to understand what's happening below here:\n",
    "num_printed = 0\n",
    "\n",
    "for token, count in sorted(beo_dict.items(), key=lambda item: -1*item[1]) :\n",
    "    print(token + \" had \" + str(count) + \" instances.\")\n",
    "    num_printed += 1\n",
    "    \n",
    "    if num_printed == 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LO,', 'people-kings', 'spear-armed', 'Danes,', 'sped,', 'heard,', 'won!', 'foes,', 'tribe,', 'mead-bench', 'tore,', 'earls.', 'friendless,', 'foundling,', 'him:', 'welkin,', 'throve,', 'folk,', 'near,', 'whale-path,', 'mandate,', 'gifts:', 'he!', 'born,', 'halls,', 'folk,', 'while;', 'him,', 'Wonder,', \"world's\", 'renown.', 'Beowulf:', '{0a}', 'him,', 'Scyld,', 'lands.', \"father's\", 'friends,', 'gift,', 'him,', 'aged,', 'days,', 'willing,', 'nigh,', 'loyal:', 'clan.', 'moment,', 'God.', \"ocean's\", 'billow,', 'clansmen,', 'them,', 'Scyld,', 'ruled....', 'ring-dight', 'vessel,', 'ice-flecked,', 'outbound,', \"atheling's\", 'barge:', 'boat,', 'breaker-of-rings,', '{0b}', 'one.', 'him.', 'battle,', 'blade:', \"o'er\", 'away.', 'gifts,', \"thanes'\", 'treasure,', 'seas,', 'child.', \"o'er\", 'standard,', 'gold-wove', 'banner;', 'him,', 'ocean.', 'spirits,', 'mood.', 'sooth,', 'halls,', \"'neath\", 'heaven,', '--', 'freight!', 'Scyldings,', 'beloved,', 'folk,', 'world,', 'heir,', 'Healfdene,', 'life,', 'sturdy,', 'glad.', 'Then,', 'one,', 'him,']\n",
      "5921\n"
     ]
    }
   ],
   "source": [
    "# Now let's count the number of tokens that have punctuation in them. \n",
    "# Python has an object that holds punctuation, so we can use that. \n",
    "from string import punctuation # usually would do this at the top\n",
    "\n",
    "# We'll use a set trick, so need punctuation in a set\n",
    "punct_set = set(punctuation)\n",
    "\n",
    "beo_tokens_punct = []\n",
    "\n",
    "for w in beo_tokens :\n",
    "    w_set = set(w)\n",
    "    overlap = w_set.intersection(punct_set)\n",
    "    \n",
    "    if len(overlap) > 0 :\n",
    "        beo_tokens_punct.append(w)\n",
    "\n",
    "        \n",
    "print(beo_tokens_punct[:100])\n",
    "print(len(beo_tokens_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you look at those tokens with punctuation, what do you notice? \n",
    "\n",
    "1. Lots of commas just stuck onto words.\n",
    "1. Some tokens that are *just* punctuation (e.g., \"--\").\n",
    "1. Lots of capitals that might not be what we want in our tokenization. \n",
    "1. A lot of tokens with punctuation. The next cell calculates the fraction. (Note that it's not unique counts yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(beo_tokens_punct)/len(beo_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Second Exercise\n",
    "\n",
    "Now let's try working with some NLTK data. Count the words (or, more precisely, tokens) in one of the first three books included in the book corpus (_Moby Dick_, _Sense and Sensibility_, and The Book of Genesis from the _Bible_).\n",
    "\n",
    "1. Pick one of the texts and assign it to a new variable. It'll have a name like `text1` before you assign it. That variable was created when we imported everything from `nltk.book`.\n",
    "1. Look at the structure of variable.\n",
    "1. Count the tokens as above. Use the `Counter` object.\n",
    "1. Display the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense = text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(sense).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pull in 10 copora when we load the books, but there are a *ton* of books we get with NLTK. Here are the ones that come with Project Gutenberg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in nltk.corpus.gutenberg.fileids() :\n",
    "    print(book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
